Metadata-Version: 2.4
Name: refund-returns-agent
Version: 0.1.0
Summary: Generic customer support refund/returns agent
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi<1.0.0,>=0.115.0
Requires-Dist: uvicorn[standard]<1.0.0,>=0.30.0
Requires-Dist: httpx<1.0.0,>=0.27.0
Requires-Dist: pydantic<3.0.0,>=2.8.0
Requires-Dist: email-validator<3.0.0,>=2.2.0
Requires-Dist: pydantic-settings<3.0.0,>=2.4.0
Requires-Dist: sqlalchemy<3.0.0,>=2.0.30
Requires-Dist: psycopg[binary]<4.0.0,>=3.2.0
Requires-Dist: python-dotenv<2.0.0,>=1.0.1
Provides-Extra: dev
Requires-Dist: pytest<9.0.0,>=8.0.0; extra == "dev"
Requires-Dist: ruff<1.0.0,>=0.6.0; extra == "dev"
Provides-Extra: train
Requires-Dist: transformers<5.0.0,>=4.44.0; extra == "train"
Requires-Dist: datasets<3.0.0,>=2.20.0; extra == "train"
Requires-Dist: accelerate<1.0.0,>=0.33.0; extra == "train"
Requires-Dist: peft<1.0.0,>=0.12.0; extra == "train"
Requires-Dist: trl<1.0.0,>=0.10.0; extra == "train"
Requires-Dist: bitsandbytes<1.0.0,>=0.43.0; extra == "train"
Requires-Dist: sentencepiece<1.0.0,>=0.2.0; extra == "train"
Requires-Dist: safetensors<1.0.0,>=0.4.0; extra == "train"

# refund-returns-agent

Portfolio-grade, end-to-end Refund/Returns Agent:
- Structured extraction from support messages
- Tool calling for order lookup, policy checks, refund computation, RMA + label creation, escalation
- Deterministic policy engine for synthetic labels/rewards
- SFT + DPO training pipeline
- Evaluation + safety suite + UI

## Current status
Checkpoint 1 scaffold is implemented:
- Python project + lint/test tooling
- Docker Compose with Postgres + FastAPI tool server
- Strict JSON-schema-like Pydantic tool interfaces
- Idempotent mutation tools + call tracing/logging

Checkpoint 2 dataset pipeline is implemented:
- Twitter/TweetSumm preprocessing into normalized JSONL
- Olist join + deterministic policy labels for synthetic cases
- Train/val/test split JSONL generation
- DPO preference pair generation from deterministic policy outcomes

Checkpoint 3 agent orchestration is implemented:
- FastAPI `agent_server` with safe tool orchestration
- Guardrails: prompt-injection/fraud-exfil refusal + missing-info behavior
- Policy-authoritative decisions (no invalid refund writes)
- Internal case summary + next-action plan output

Checkpoint 4 evaluation is implemented:
- Offline eval harness (decision correctness + tool validity/sequence/efficiency)
- Safety/adversarial suite (injection, fraud, exfiltration, PII leakage checks)
- Human evaluation protocol + rubric for 25-case review

Checkpoint 5 SFT training is implemented:
- Deterministic SFT data formatting from synthetic cases + TweetSumm
- QLoRA training script with model presets and swap support
- LoRA adapter export/merge script

## Quickstart
1. Create env file:
   ```bash
   cp .env.example .env
   ```
2. Start services:
   ```bash
   docker compose up --build
   ```
3. Health check:
   ```bash
   curl http://localhost:8001/health
   curl http://localhost:8002/health
   ```

## Repo layout
```
refund-returns-agent/
  services/
    tool_server/
    agent_server/
    ui/
  data/
    raw/
    processed/
  pipelines/
  training/
  eval/
  infra/db/
  tests/
```

## Checkpoint 2 data pipeline
Expected raw dataset placement:
- `data/raw/olist/olist_orders_dataset.csv`
- `data/raw/olist/olist_order_items_dataset.csv`
- `data/raw/olist/olist_customers_dataset.csv`
- `data/raw/olist/olist_order_payments_dataset.csv`
- `data/raw/twitter/twcs.csv` (optional but recommended)
- `data/raw/tweetsumm/*.jsonl|*.csv` (optional)

Run preprocessing:
```bash
python3 pipelines/preprocess_text.py --raw-dir data/raw --processed-dir data/processed --max-rows 200000
```

Build synthetic dataset:
```bash
python3 pipelines/build_dataset.py --raw-dir data/raw --processed-dir data/processed --max-cases 5000 --seed 42
```

Outputs:
- `data/processed/twitter_support_texts.jsonl`
- `data/processed/tweetsumm_pairs.jsonl` (if TweetSumm files are present)
- `data/processed/synthetic_cases_train.jsonl`
- `data/processed/synthetic_cases_val.jsonl`
- `data/processed/synthetic_cases_test.jsonl`
- `data/processed/dpo_pairs_train.jsonl`

## Checkpoint 3 agent server
Run stack:
```bash
docker compose up --build
```

Agent inference request:
```bash
curl -X POST http://localhost:8002/agent/respond \\
  -H \"Content-Type: application/json\" \\
  -d '{
    \"case_id\": \"CASE-DEMO-1\",
    \"customer_message\": \"My item arrived damaged. Please refund order ORD-1001.\",
    \"order_id\": \"ORD-1001\"
  }'
```

Expected behavior:
- Runs allowlisted tool sequence only.
- If policy says not eligible: denies, no `create_return/create_label` calls.
- If missing evidence: asks for required info, no write tools.
- Fraud/exfiltration prompts: refusal with no tool calls.

## Checkpoint 4 evaluation
Run offline eval (requires `agent_server` running):
```bash
python3 eval/eval_harness.py \
  --dataset data/processed/synthetic_cases_test.jsonl \
  --agent-url http://localhost:8002 \
  --limit 200 \
  --output eval/results/eval_report.json
```

Run safety suite:
```bash
python3 eval/safety_suite.py \
  --agent-url http://localhost:8002 \
  --output eval/results/safety_report.json
```

Expected outputs:
- `eval/results/eval_report.json` with:
  - `decision_accuracy`
  - `tool_validity_rate`
  - `sequence_correct_rate`
  - `efficiency_rate`
  - `avg_calls_per_episode`
- `eval/results/safety_report.json` with per-case pass/fail and pass rate.

## Checkpoint 5 SFT (QLoRA)
Install training extras:
```bash
python3 -m pip install -e '.[train]'
```

Prepare SFT datasets only:
```bash
python3 training/sft_train.py \
  --prepare-only \
  --train-cases data/processed/synthetic_cases_train.jsonl \
  --val-cases data/processed/synthetic_cases_val.jsonl \
  --tweetsumm data/processed/tweetsumm_pairs.jsonl \
  --prepared-train data/processed/sft_train_prepared.jsonl \
  --prepared-val data/processed/sft_val_prepared.jsonl
```

Run QLoRA SFT (CUDA host):
```bash
python3 training/sft_train.py \
  --model mistral-7b-instruct-v0.2 \
  --train-cases data/processed/synthetic_cases_train.jsonl \
  --val-cases data/processed/synthetic_cases_val.jsonl \
  --tweetsumm data/processed/tweetsumm_pairs.jsonl \
  --output-dir models/sft_qlora \
  --batch-size 1 \
  --grad-accum 16 \
  --learning-rate 2e-4 \
  --num-epochs 1 \
  --max-seq-len 1024
```

Export merged model:
```bash
python3 training/export_model.py \
  --base-model mistralai/Mistral-7B-Instruct-v0.2 \
  --adapter-dir models/sft_qlora/adapter \
  --output-dir models/sft_merged
```
